{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weights of the Model\n",
    "\n",
    "The initialization of neural network weights using a standard deviation of \\(\\sqrt{1/n}\\) (where \\(n\\) is the number of input neurons, also known as fan-in) is a strategy designed to maintain the variance of the outputs of each neuron at initialization. Let's delve into a mathematical explanation and derivation of why this specific value is chosen, particularly in the context of keeping the variance of the outputs stable.\n",
    "\n",
    "### Background\n",
    "When we initialize the weights of a neural network, we want to ensure that the signal (i.e., the output of each neuron before applying the activation function) does not vanish or explode as it propagates through the network. This stability helps in maintaining effective gradient propagation during training.\n",
    "\n",
    "### Assumptions\n",
    "- The weights \\( w_{ij} \\) are initialized independently from a normal distribution with mean 0 and standard deviation \\(\\sigma\\).\n",
    "- Each neuron receives \\( n \\) inputs \\( x_i \\), which are also assumed to be independent and have a mean of 0 and some constant variance (say, variance = 1 for simplicity).\n",
    "\n",
    "### Output Variance Calculation\n",
    "Consider a neuron's output \\( z \\) before applying the activation function, calculated as:\n",
    "\\[ z = \\sum_{i=1}^n w_i x_i \\]\n",
    "where \\( w_i \\) are the weights and \\( x_i \\) are the inputs.\n",
    "\n",
    "#### Step 1: Calculate the Variance of \\( z \\)\n",
    "Since the weights and inputs are independent and assuming the inputs also have zero mean, the variance of the product \\( w_i x_i \\) for each \\( i \\) is simply the product of their variances (due to independence and zero means):\n",
    "\\[ \\text{Var}(w_i x_i) = \\text{Var}(w_i) \\cdot \\text{Var}(x_i) = \\sigma^2 \\cdot 1 = \\sigma^2 \\]\n",
    "\n",
    "Since \\( z \\) is the sum of \\( n \\) such independent terms \\( w_i x_i \\), the variance of \\( z \\) is the sum of their variances:\n",
    "\\[ \\text{Var}(z) = \\text{Var}\\left(\\sum_{i=1}^n w_i x_i\\right) = \\sum_{i=1}^n \\text{Var}(w_i x_i) = n\\sigma^2 \\]\n",
    "\n",
    "#### Step 2: Desired Variance of \\( z \\)\n",
    "To maintain the variance of the output \\( z \\) similar to the variance of the input across layers, we would like \\(\\text{Var}(z) = 1\\). This condition helps prevent the vanishing or exploding gradients during training.\n",
    "\n",
    "Setting \\( \\text{Var}(z) = 1 \\):\n",
    "\\[ n\\sigma^2 = 1 \\]\n",
    "\\[ \\sigma^2 = \\frac{1}{n} \\]\n",
    "\n",
    "Therefore, the standard deviation \\( \\sigma \\) should be:\n",
    "\\[ \\sigma = \\sqrt{\\frac{1}{n}} \\]\n",
    "\n",
    "### Conclusion\n",
    "This derivation shows that setting the standard deviation of the weight initialization to \\(\\sqrt{1/n}\\) ensures that the output of each neuron has a variance of 1, assuming the inputs also have a variance of 1. This balance is crucial for maintaining effective learning, as it prevents the scale of the neuron outputs from increasing or decreasing dramatically across layers, which can lead to numerical instability or poor convergence. This is why the \\(\\sqrt{1/n}\\) factor is commonly used in weight initialization methods like Xavier/Glorot initialization (which adjusts the variance further based on both the number of inputs and outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
