{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refreshers on Derivatives of Vector Valued Functions\n",
    "\n",
    "A Function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is differentiable at $a \\in \\mathbb{R}^n$ if there is an $m \\times n$ matrix such that:\n",
    "\n",
    "$$ \\lim_{x \\to a} \\frac{|f(x) - f(a) - A \\cdot (x-a) |} {|x-a|} = 0$$ \n",
    "\n",
    "If such matrix exists, the matrix $A$ is denoted by $Df(a)$ and is called the Jacobian.\n",
    "\n",
    "Note that $|x-a|$ is the distance metric defined by the Uclidean Distance $\\sqrt{{(x_1 - a_1)}^2 + {(x_2 - a_2)}^2 .. {(x_n-a_n)}^2}$ and is a real valued scalar.\n",
    "\n",
    "Formally, this can be derived from the general definiton of a derviative:\n",
    "\n",
    "$$\n",
    "f'(a) = \\lim_{x\\to a} \\frac{f(x) - f(a)}{x-a}\n",
    "$$\n",
    "Where this is only true if and only if:\n",
    "\n",
    "$$\n",
    "0= \\lim_{x\\to a} \\frac{f(x) - f(a)}{x-a} - f'(a)\n",
    "$$\n",
    "\n",
    "which can be transformed to:\n",
    "\n",
    "$$\n",
    "0= \\lim_{x\\to a} \\frac{f(x) - f(a) - f'(a)(x-a)}{x-a}\n",
    "$$\n",
    "\n",
    "and thus the evaluated to the final distance of each numerator and denominator from the origin:\n",
    "\n",
    "$$\n",
    "0= \\lim_{x\\to a} \\frac{|f(x) - f(a) - f'(a)(x-a)|}{|x-a|}\n",
    "$$\n",
    "(Since the notion of divison of two vectors is silly)\n",
    "\n",
    "Here $f'(a)$ represents our Jacobian matrix in $m \\times n$ shape. Denote I refer to this matrix from now on as $Df(x)$, though in the example above\n",
    "it is being used to be evaluated at point $a$ in vector space.\n",
    "\n",
    "\n",
    "### Defining the Jacobian in terms of coordinates and Indices\n",
    "\n",
    "#### Definitions of Jacobians via multiple functions of $f$\n",
    "Let the function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ be given by the m differentiable functions $f_1(x_,..,x_n),\\dots, f_m(x_1,\\dots, x_n)$ such that:\n",
    "\n",
    "$$\n",
    "f(x_1,\\dots,x_n) = \\begin{bmatrix}\n",
    "f_1((x_,..,x_n)) \\\\\n",
    "\\vdots \\\\\n",
    "\\vdots \\\\\n",
    "f_m((x_,..,x_n))\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Supposing we can represent each $f$ as a family of functions, indexed from 1 to $m$, we can take the derivative of each function $f_i$ for $i \\in 1\\dots m$:\n",
    "\n",
    "$$\n",
    "Df_i(x_1,...,x_n) \\to \\hat{v_i} \\ \\text{such that} \\ v_i \\in \\mathbb{R}^n\n",
    "$$\n",
    "In this case, we know $v_i$ to be $n$ dimensional, because of our original formulation of the derivative of vector valued functions. Note that we must compute\n",
    "$f'(a)$ which is another function $Df(a)$. However, we need the rows of $f'(a)$ (a linear map of sorts) for each ith row in $A$ to represent a tangent line. This \n",
    "tangent line is conditioned to be for an input vector valued to be resultant vector of $x-a$. \n",
    "\n",
    "Similar to our single valued derivative case (grade school calculus):\n",
    "$$\n",
    "y = f(a) + f'(a)(x-a)\n",
    "$$ \n",
    "where the above function is the tangent line of some differentiable function at some point $a$ for a function $y=f(x)$ (Note this function is *linear*),\n",
    "\n",
    "we want to build this same representation for a multivalued function $f_i(x_1,...,x_n)$. Thus we rely on partial derivatives to represent individual *linear* tangent lines\n",
    "with respect to each individual input $x_j$ where $j \\in 1...n$. Thus, we can represent each row of $Df(x)$ as:\n",
    "$$\n",
    "Df_i(x_1,..., x_n) = \\left (\\frac{\\partial f_i}{\\partial x_1} ,\\frac{\\partial f_i}{\\partial x_2}, \\cdots, \\frac{\\partial f_i}{\\partial x_n}\\right)\n",
    "$$\n",
    "So as a result, we can say $Df_i(x_1,...,x_n)$ when applied to the elements of $x-a$, will represent the linear approximations of wiggles on the vector valued function $f$ that will \n",
    "be approximately zero in distance from if we took the difference of $f(x) - f(a)$ exactly.\n",
    "\n",
    "As a result, we can expand this out to each function $f_1...f_m$ in $f$ so that $Df(x_1,...,x_n)$ our Jacobian is:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "Df(x) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Defining coordinates from domain and codomain dimensions\n",
    "In our case, we will want to have a conveinient notation for handling indices in our Jacobian matrix, and tracking our output vector's dimensions, along with input vector's dimensions.\n",
    "\n",
    "### Defining Differentials of Compositions of Functions\n",
    "\n",
    "Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ and $g: \\mathbb{R}^m \\rightarrow \\mathbb{R}^l$ be differentiable functions. Also there is a composition function:\n",
    "$$\n",
    "g \\circ f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^l\n",
    "$$\n",
    "that is also differentiable with a derivative given by: if $f(a)=b$, then\n",
    "$$\n",
    "D(g\\circ f)(a) = D(g)(b) \\cdot D(f)(a)\n",
    "$$\n",
    "By the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network from an MLP with ReLU and NLL Loss\n",
    "\n",
    "Multi-Layer Perceptron (MLP) with ReLU (Rectified Linear Unit) activation are common a neural network architecture, along with \n",
    "a Negative Log-likelihood loss (NLL Loss) for handling multiclass output.\n",
    "\n",
    "##### What does an MLP represent mathematically?\n",
    "\n",
    "An MLP is a neural network architecture where we have layers of preceptrons (sometimes called neurons) which can be seen as   \n",
    "connected groups of bipartite graphs, such that each layer of nodes (neurons) is not connected to any other node in it's layer. Each neuron recieves  \n",
    "inputs from all neurons from the layer previous to it. More specifically, each neuron in each layer takes all of the neuron inputs and multiplies  \n",
    "itself with a weight for each incoming input. These are all summed together. Each neuron in the layer then passes its summed value to  \n",
    "an activation function (such as the ReLU in our example). This will be our neuron's final output.\n",
    "\n",
    "### Let's Define it Better!\n",
    "\n",
    "As noted, we often model it as a series of layers where each layer   \n",
    "is composed of neurons. Each neuron computes a weighted sum of its inputs, which is then passed through an activation function.   \n",
    "Here's a simplified LaTeX representation that captures these components across multiple layers:\n",
    "\n",
    "Consider an MLP with one hidden layer and an output layer. The model consists of an input vector, hidden layers with activation functions, and an output layer. We will denote:\n",
    "- $ \\mathbf{x} $ as the input vector.\n",
    "- $ \\mathbf{W}^{(l)} $  as the weights of layer \\( l \\), respectively.\n",
    "- $ \\sigma $ as the activation function (e.g., sigmoid, ReLU).\n",
    "\n",
    "#### Hidden Layer\n",
    "\n",
    "Let's say the MLP has one hidden layer. The output of this layer for each neuron \\( j \\) in the hidden layer can be represented as:\n",
    "$$\n",
    "a_j^{(1)} = \\sigma\\left(\\sum_{i=1}^{n} W_{ji}^{(1)} x_i\\right)\n",
    "$$\n",
    "where $ n $ is the number of inputs to each neuron, $ W_{ji}^{(1)} $ are the weights connecting input $ i $ to neuron $ j $ in the first hidden layer.\n",
    "\n",
    "#### Output Layer\n",
    "\n",
    "For the output layer, if we have $ k $ outputs, the output for each neuron $k$ in the output layer can be represented as:\n",
    "$$\n",
    "y_k = \\sigma\\left(\\sum_{j=1}^{m} W_{kj}^{(2)} a_j^{(1)} \\right)\n",
    "$$\n",
    "where $ m $ is the number of neurons in the hidden layer, $ W_{kj}^{(2)} $ are the weights connecting the hidden layer neuron $j$ to the output neuron $k$.\n",
    "\n",
    "\n",
    "Notice that $(j)$ represents the logit value for a neuron, which is a row of the matrix $\\mathbf({W}^{(l)}$. If we iterate over every input neuron we can store  \n",
    "each value in a vector $[a_1, a_2, a_3.., a_k]$. This can thus be represented as simple a linear transformation! We will show more in the next section.\n",
    "\n",
    "**To Underscore**, we will reference this indexing later in this writeup. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Defining Operations Concretely\n",
    "\n",
    "Suppose you have an input $\\mathbf{x}$, weights $ \\mathbf{W} $, (and biases whidh is common but we leave it out $ \\mathbf{b} $). The operations in the layer can be described as:\n",
    "\n",
    "1. **Linear Transformation**: $ \\mathbf{z} = \\mathbf{Wx}$\n",
    "2. **ReLU Activation**: $ \\mathbf{a} = \\text{ReLU}(\\mathbf{z}) $, where $ \\text{ReLU}(z_i) = \\max(z_i, 0) $\n",
    "3. **NLL Loss**: $ \\mathbf{L} = \\text{NLLLoss}(\\mathbf{z}) = -\\log(z_t) $ \n",
    "\n",
    "From our previous example, we can cleanly represent our Input to Output as a Linear Transformation, rather than a combersome summation.  \n",
    " We will reference this from now on for the forward pass of MLP, and come back to this notation later when things get more tricky.\n",
    "\n",
    "### Example of a Forward Pass\n",
    "We will show an example of a forward pass that works on a simple network, with two layers, and two neurons, three inputs and  \n",
    "two outputs.\n",
    "\n",
    "Assume for our example, we have: \\\n",
    "  - **Input** $\\mathbf{x}$ = $[ 0, 1, 2]$\n",
    "  - **Weights**:\n",
    "    - $\\mathbf{W_1} = \\left[\\begin{smallmatrix}1&2\\\\3&4\\\\5&6\\end{smallmatrix}\\right]$\n",
    "    - $\\mathbf{W_2} = \\left[\\begin{smallmatrix}1&0\\\\0&1\\end{smallmatrix}\\right]$\n",
    "- **Label**: $t = 3$\n",
    "\n",
    "\n",
    "#### The Forward Pass\n",
    "\n",
    "1. **Layer 1**: Input $ \\mathbf{x} $, weights $ \\mathbf{W}_1 $\n",
    "   - Linear transformation: $ \\mathbf{z}_1 = \\mathbf{W}_1 \\mathbf{x}  $\n",
    "   - Activation: $ \\mathbf{a}_1 = \\text{ReLU}(\\mathbf{z}_1) $\n",
    "2. **Output Layer**: Input: $ \\mathbf{a_1} $, weights $ \\mathbf{W}_2 $\n",
    "   - Linear transformation: $ \\mathbf{z_2} = \\mathbf{W}_2 \\mathbf{a}_1 $\n",
    "   - Activation: $ \\mathbf{a}_2 = \\text{ReLU}(\\mathbf{z}_2) $\n",
    "3. **Computing the Loss**: Input activations $ \\mathbf{a_3}$, labels $\\mathbf{t}$\n",
    "   - Loss computation: $\\mathbf{L} = -\\log(\\mathbf{a}_t) \\text{\\ where \\ } \\mathbf{a}_t = \\text{element at index \\ } t \\text{\\ of \\ } \\mathbf{a} $\n",
    "   - $t$ here represents the class label index of the input example. As a result the possible values must \n",
    "    be an index into the shape of $\\mathbf{a_3}$. e.g. if $\\mathbf{a}_3 \\in \\mathbb{R}^3$, $t \\in 1...3$\n",
    "\n",
    "### Quality of Life Reformulations\n",
    "Given how that we have set up the premise, we can go ahead and try to use our previous mathematical foundation to reformulate some specific parts of our forward pass and the operations themselves. Why do we do this? Well we want to make it easier to convince oneself of how these models update or learn (more on this in the [gradients and backpropogation](#backward-pass-reverse-mode) section)\n",
    "\n",
    "#### Representing MLP transformations in a more convenient Way\n",
    "\n",
    "We currently represent each linear transformation for each layer as a Matrix-Vector multiplication. We find this quite cumbersome to handle\n",
    "when we want to do operations later on, specifically when trying to differentiate our composed function calls.\n",
    "\n",
    "We can represent a linear transformation $ \\mathbf{z}_l = \\mathbf{W}_l \\mathbf{x}  $ as:\n",
    "$$\n",
    "f_{W_l}\\mathbf{x}) \\ \\text{where \\ } f_{\\mathbf{W}_l}: \\mathbb{R}^n \\to \\mathbb{R}^m\n",
    "$$\n",
    "\n",
    "Now, our function is not necessarily multivalued, but represents a frozen transformation on vector valued inputs.\n",
    "Looking back to our refresher on vector valued functions, this means we can focus on differentiating $f$, the linear transformation directly.\n",
    "This also means we can invoke this function for inputs that are not just vectors, but matrices or tensors, representing collections of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Gradients of our MLP\n",
    "\n",
    "Neural Networks learn by updating it's weights against the the errors of the network to the correct answer. To determine what might need to change for the models parameters (weights $W^{(l)}$ for us), the training algorithm performs what is called a backwards pass on the network.\n",
    "\n",
    "The backward pass in a neural network, particularly for a Multi-Layer Perceptron (MLP) with ReLU (Rectified Linear Unit) activation, involves computing the gradient of the loss function with respect to the weights. This process allows for the updating of parameters via optimization algorithms like gradient descent. Here's how it works for a MLP layer with ReLU activation:\n",
    "\n",
    "\n",
    "### Backward Pass (Reverse Mode)\n",
    "\n",
    "The gradients need to be computed with respect to $ \\mathbf{W} $. Assuming that the gradient of the loss $ L $ with respect to the output of this layer $ \\mathbf{a} $ is known (denoted as $ \\frac{\\partial L}{\\partial \\mathbf{a}} $), the gradients can be calculated as follows:\n",
    "\n",
    "1. **Gradient through ReLU**:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{z}} = \\frac{\\partial L}{\\partial \\mathbf{a}} \\odot \\text{ReLU}'(\\mathbf{z})\n",
    "   $$\n",
    "   Here, $\\text{ReLU}'(z_i)$ is the derivative of ReLU, which is 1 for $ z_i > 0 $ and 0 otherwise. \n",
    "   This can be proved simply by:\n",
    "   $$\n",
    "   \\frac{\\partial \\text{RELU}}{\\partial \\mathbf{z}} = \\frac{\\partial \\left( \\begin{cases} z_i & z_i > 0 \\\\ 0 \\end{cases}\\right)}{\\partial \\mathbf{z}}\n",
    "   $$\n",
    "   Moving the derivative inside the piece wise:\n",
    "    $$\n",
    "   \\frac{\\partial \\text{RELU}}{\\partial \\mathbf{z}} = \\left( \\begin{cases} \\frac{z_i}{z_i} & z_i > 0 \\\\ 0 \\end{cases}\\right) = \\left( \\begin{cases} 1 & z_i > 0 \\\\ 0 \\end{cases}\\right)\n",
    "   $$\n",
    "\n",
    "2. **Gradient w.r.t. Weights**:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{z}} \\cdot \\mathbf{x}^T\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{z}} \\quad (\\text{summing over the batch if needed})\n",
    "   $$\n",
    "\n",
    "3. **Gradient through NLL Loss w.r.t. logits $z_i$**:\n",
    "   \n",
    "   Given that NLL Loss takes as input a vector and returns a value, $\\text{NLL}: \\mathbb{R}^c \\to \\mathbb{R} $, we can apply a similar proof strategy to define a jacobian that will generate a map for some loss $L$ back to the given logits in $\\mathbf{z_i}$.  \n",
    "\n",
    "   First we just define roughly the inner function with respect to potential input logits:\n",
    "   $$ \\frac{\\partial \\text{NLL}_t}{\\partial z_i} = \\frac{\\partial}{\\partial z_i}\\left(\\begin{cases} -\\log(z_t) & t = i \\\\ 0 & i \\ne t \\end{cases}\\right) $$  \n",
    "   $$ \\frac{\\partial \\text{NLL}_t}{\\partial z_i} = \\begin{cases} -\\frac{1}{z_t} & t = i \\\\ 0 & i \\ne t \\end{cases} $$\n",
    "   where we are fixing in some $t$ index baked into the function (like a closure).\n",
    "\n",
    "   Then the Jacobian can be seeing as: $\\text{DNLL}_t(z): \\mathbb{R} \\to \\mathbb{R}^c$ such that:\n",
    "   $$\n",
    "   \\begin{bmatrix}\n",
    "   \\frac{\\partial \\text{NLL}_t}{\\partial z_1} \\\\\n",
    "   \\frac{\\partial \\text{NLL}_t}{\\partial z_2} \\\\ \\vdots \\\\\n",
    "   \\frac{\\partial \\text{NLL}_t}{\\partial z_n}\n",
    "   \\end{bmatrix} * z\n",
    "   $$\n",
    "   \n",
    "   In our implementation, we end using simply $loss = -input[target]$ mainly due to the fact that, when simplifying the gradients,\n",
    "   we see:\n",
    "   $$ \\text{NLL}(x,y) = -\\log(p_y) = \\log\\left(\\frac{e^z_t}{\\sum_j{e^z_j}}\\right) = z_y - \\log\\left(\\sum_j{e^z_j}\\right) \n",
    "   $$\n",
    "   Where loss is: $-z_y + \\left(\\sum{e^z_j}_j\\right)$, taking the derivative of this function results in $-1$ for the target index. Since in our\n",
    "   case NL_Loss is the last layer and our chainrule starts with this value, we can assume that we don't need the $log(p_z)$, and allow gradients to flow\n",
    "   directly from this function. **Note, our loss value will be different numerically** because we are going to be computing raw logit outputs, and not softmax'd outputs. But, for the sake of gradient flow back, it will be equivalent, as $-1/z_i$ and $-1$ are the same if the LHS multiplicative will be $-1$ (i.e. $\\frac{-1}{-1}$).\n",
    "\n",
    "   We want to derive the loss **with logits** and understand what the derivative actually looks like when not on raw logits.\n",
    "\n",
    "   Recall: $-\\log(p_y)$ where $p_j$ actually represents values after a softmax function applied. Seperately, a softmax function can be written as:\n",
    "\n",
    "   $$\n",
    "   p_y = \\frac{e^z_y}{\\sum_j{e^z_j}}\n",
    "   $$\n",
    "\n",
    "   We want to get the ∂p/∂z(derivative of softmax):\n",
    "\n",
    "   $$\n",
    "   \\text{given:} f_y(\\mathbf{z}) = f_y(z_1, ..., z_n) = p_y  = \\frac{e^z_y}{\\sum_j{e^z_j}} \\text{ \\ where \\ } z \\in \\mathbb{R}^n\n",
    "\n",
    "   \\text{Define a vector-valued function \\ } f: f = \\begin{bmatrix} f_1(z_1,...,z_n) \\\\ f_1(z_1,...,z_n) \\\\ \\vdots \\\\ f_n(z_1,...,z_n) \\end{bmatrix} \n",
    "   $$\n",
    "   we want to find $Df_i(\\mathbf{z})$, which can represented as:\n",
    "   $$\n",
    "   Df_ij(z) \\in R\n",
    "   Df(z) \\in R^(nxn)\n",
    "\n",
    "   Df_ij: R^n  -> R\n",
    "   $$\n",
    "   $$\n",
    "   Df_ij(\\mathbf{z}) = Df_ij(z_1,...,z_n) = \\frac{\\partial \\frac{ e^z_i}{ \\sum_k{e^z_k}}}{\\partial z_j}\n",
    "   = \\begin{cases} 0 & i \\ne j  \\\\   & i = j\\end{cases}\n",
    "    where i \\ne j, \\\\ \n",
    "    e^z_i/ sum(e^z_k) where k = j e^(z_i) / (e^z_1 + e^z_2 ... + e^z_n) = e^z_i * ((e^z_1 + e^z_2 ... + e^z_n)^-1) -> e^z_i *  (-(e^z_1 + e^z_2 ... + e^z_n)^-2) * e^z_j\n",
    "\n",
    "   Df_ij(z_1,...,z_n) = e^z_i *  (-(e^z_1 + e^z_2 ... + e^z_n)^-2) * e^z_j = -(e^(z_i) * e^z_j/ ((e^z_1 + e^z_2 ... + e^z_n)^-2)) = -1e^(z_i) * e^z_j/(sum_k(e^z_k))* sum(e^z_k))\n",
    "   \n",
    "   $$\n",
    "   $$\n",
    "   =  -\\frac{e^(z_i) * e^z_j}{(\\sum_k{e^z_k})* \\sum_k{e^z_k})}\n",
    "    = -p_i * p_j\n",
    "   $$\n",
    "   When i = j\n",
    "   $$\n",
    "   - p_i - pi^2\n",
    "   $$\n",
    "   $$\n",
    "    1/ g(x) = a * g(x)^-1 -derive-> -g(x)^-2 * g'(x)\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Deriving the gradients w.r.t. the weights $W$\n",
    "\n",
    "To understand the geometric interpretation and prove the derivative of a loss function \\( L \\) with respect to a weights matrix \\( W \\) in the context of neural networks, we primarily use concepts from multivariable calculus and matrix calculus. The proof here will follow the fundamental principle that the derivative of a function at a point gives the best linear approximation to the function at that point, indicating the direction and rate of steepest ascent from that point.\n",
    "\n",
    "### Setting Up the Scenario\n",
    "\n",
    "Assume $L$ is a scalar loss function that depends on the output $y$ of a neural network, where $ y $ itself is a function of the weights matrix $ W $. Suppose the neural network follows as described in the forward pass:\n",
    "\n",
    "$$\n",
    "y = f(Wx)\n",
    "$$\n",
    "\n",
    "Here, $f$ represents the activation function applied element-wise, $x$ is the input vector. The output $y$ thus depends on the weights matrix $W$.\n",
    "\n",
    "### The Geometric Definition of Derivatives\n",
    "\n",
    "In the context of functions from $ \\mathbb{R}^n \\to \\mathbb{R}^m $, the derivative is a linear map that best approximates the change in the function near a point. For vector-valued functions $ F: \\mathbb{R}^n \\to \\mathbb{R}^m $, the derivative at a point is represented by the Jacobian matrix, which consists of all first-order partial derivatives as described before.\n",
    "\n",
    "For our case, $ L: \\mathbb{R}^{m} \\to \\mathbb{R} $, the derivative with respect to the matrix $ W $ (which is itself a tensor) needs to capture how infinitesimal changes in the elements of $ W $ affect the change in $ L $. We also know that $ W $ can be represented as a linear map $ W: \\mathbb{R}^n \\to \\mathbb{R}^m$. We can **treat $W$ as a function**.\n",
    "\n",
    "### Derivative Computation\n",
    "\n",
    "1. **Recollect the Forward pass as**:\n",
    "   $$\n",
    "   z = W(x) \n",
    "   $$\n",
    "   $$\n",
    "   y = f(z)\n",
    "   $$\n",
    "   $$\n",
    "   L = \\text{Loss}(y, \\text{target})\n",
    "   $$\n",
    "\n",
    "2. **Applying the Chain Rule**:\n",
    "   The derivative of $L$ with respect to $W$ involves applying the chain rule through the layers of operations:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial W}\n",
    "   $$\n",
    "\n",
    "   - $ \\frac{\\partial L}{\\partial y} $ is the gradient of the loss with respect to the output $y$, which depends on the specific loss function used.\n",
    "   - $ \\frac{\\partial y}{\\partial z} $ is the derivative of the activation function $f$, evaluated element-wise at $z$.\n",
    "   - $ \\frac{\\partial z}{\\partial W} $ captures how changes in $W$ affect $z$, and for a given element $ z_i = \\sum_j W_{ij} x_j $, the derivative with respect to each element $ W_{ij} $ is simply $ x_j $.\n",
    "\n",
    "3. **Jacobian to Matrix Form**: \n",
    "   As before, $\\frac{\\partial z}{\\partial W} $ catpures $ z_i = \\sum_j W_{ij} x_j $, the derivative with respect to each element $ W_{ij} $ is simply $ x_j $.\n",
    "   Representing this as a vector valued function would first look like:\n",
    "   $$\n",
    "   W(\\mathbf{z}) = W(x_1,x_2,...,x_n) = \\begin{pmatrix} W_1(x_1,x_2,...,x_n) \\\\ W_2(x_1,x_2,...,x_n) \\\\ \\vdots \\\\ W_m(x_1,x_2,...,x_n)\\end{pmatrix}\n",
    "   $$\n",
    "   where, the function $W_j$ is from $j$ functions that come from $W$ such that $j \\in 1...m$. Furthermore we can think of a function $W_j$ as:\n",
    "   $$\n",
    "   z_j = W_j(x_1,x_2,...,x_n) = W_{j1}*x_1 + W_{j2}*x_2 + ... + W_{jn} *x_n = \\sum_k W_{jk}x_k\n",
    "   $$  \n",
    "   Now looking at $\\frac{\\partial z}{\\partial W}$, we want to build the differential $DW(x)$ very similar to our derviation in the refresher. \n",
    "\n",
    "   $$\n",
    "   Df_i(x_1,..., x_n) = \\left (\\frac{\\partial f_i}{\\partial x_1} ,\\frac{\\partial f_i}{\\partial x_2}, \\cdots, \\frac{\\partial f_i}{\\partial x_n}\\right)\n",
    "   $$\n",
    "   except, **we want to take the partial derivatives with respect, $W_ik$** instead of {x_k}.\n",
    "   $$\n",
    "   Df_j(x_1,x_2,...,x_n) = \\left (\\frac{\\partial f_i}{\\partial W_{j1}} ,\\frac{\\partial f_i}{\\partial W_{j2}}, \\cdots, \\frac{\\partial f_i}{\\partial W_{jn}}\\right)\n",
    "   $$\n",
    "   And here, we replace $f_j$ with our indexed functions from above $W_j(\\mathbf{x})$.\n",
    "   $$\n",
    "   DW_j(x_1,x_2,...,x_n) = \\left (\\frac{\\partial z_i}{\\partial W_{j1}} ,\\frac{\\partial z_i}{\\partial W_{j2}}, \\cdots, \\frac{\\partial z_i}{\\partial W_{jn}}\\right)\n",
    "   $$  \n",
    "   Notice that we replaced $f_i$ with $z_i$ as in our original derivation $f_i$ represents the output as well, so we make that substitution here.\n",
    "   \n",
    "    \n",
    "   Now, to map it back to our partial derivative $\\frac{\\partial z}{\\partial W}$,\n",
    "   $$\n",
    "   z_j * \\frac{\\partial}{\\partial W_j} = W_j(x_1,x_2,...,x_n) * \\frac{\\partial}{\\partial W_j} \\\\\n",
    "   \\frac{\\partial z_j}{\\partial W_j} =  \\frac{\\partial W_j(x_1,x_2,...,x_n)}{\\partial W_j} $$\n",
    "   Here we substitute in $DW_j$ as shown before, $\\frac{\\partial z_j}{\\partial W_j} = DW_j$\n",
    "   $$\n",
    "   \\frac{\\partial z_j}{\\partial W_j} =  DW_j(x_1,x_2,...,x_n)\n",
    "   $$\n",
    "   Note that $DW_j$ is a function, representing the infinitesimal changes of the elements of $W$ with respect to the output $z_j$ at some point $\\mathbf{x}$.\n",
    "\n",
    "   We can now map it to the Jocabian, given that we have each function $W_j(\\mathbf{x})$\n",
    "   $$\n",
    "   DW(\\mathbf{x}) = \\begin{bmatrix} \n",
    "   \\frac{\\partial z_1}{\\partial W_11} & \\frac{\\partial z_1}{\\partial W_12} & \\cdots & \\frac{\\partial z_1}{\\partial W_1n} \\\\\n",
    "   \\frac{\\partial z_2}{\\partial W_21} & \\frac{\\partial z_2}{\\partial W_22} & \\cdots & \\frac{\\partial f_2}{\\partial W_2n} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   \\frac{\\partial z_m}{\\partial W_m1} & \\frac{\\partial z_m}{\\partial W_m2} & \\cdots & \\frac{\\partial z_m}{\\partial W_mn}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Each entry here direclty maps to a value in the original summation defined: \n",
    "   \n",
    "   Expanding $ \\frac{\\partial z}{\\partial W} $ in matrix terms, we have that each component $ \\frac{\\partial z_i}{\\partial W_{ij}} = x_j $. \n",
    "   \n",
    "   Note as well that we can expand out  $ \\frac{\\partial z}{\\partial \\mathbf{x}} $ as well: \n",
    "   $ z_i = \\sum_j W_{ij} x_j $ can be used to differentiate $z_i$ w.r.t either $W$ or $x$ so that: $ \\frac{\\partial z_i}{\\partial x_{j}} = W_{ij} $\n",
    "   \n",
    "   \n",
    "   Thus, we can transform the above jacobian representation in terms of matrix operations:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot x^T\n",
    "   $$\n",
    "\n",
    "   Here, $ \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} $ is treated as a row vector due to broadcasting rules in matrix multiplication, and $x^T$ is the transpose of the input vector $ x $.\n",
    "\n",
    "## Generalizing to an Arbitrarily Deep and Wide MLP\n",
    "\n",
    "### Network Setup and Notation\n",
    "\n",
    "Assume a neural network with $ K $ layers. Each layer $ k $ has:\n",
    "- A weights matrix $W_k$\n",
    "- An activation function $f_k$\n",
    "- An input $x_k$\n",
    "- An output $y_k$, where $y_k = f_k(z_k) $ and $z_k = W_k x_k $\n",
    "\n",
    "The output $ y_k $ of each layer serves as the input $ x_{k+1} $ to the next layer.\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "1. **Input Layer**: $ x_1 $ is the input to the network.\n",
    "2. **Hidden Layers and Output**:\n",
    "   For each layer $ k $:\n",
    "   $$\n",
    "   z_k = W_k x_k\n",
    "   $$\n",
    "   $$\n",
    "   y_k = f_k(z_k) \\quad (\\text{with } x_{k+1} = y_k \\text{ for the next layer})\n",
    "   $$\n",
    "3. **Final Output**: The output $y_K$ of the last layer is used to compute the loss $ L $ based on a target label e, i.e.,$ L = \\text{Loss}(y_K, \\text{target}) $.\n",
    "\n",
    "### Backward Pass (General Case)\n",
    "\n",
    "To compute the gradient $ \\frac{\\partial L}{\\partial W_k} $ for each layer's weights $ W_k $, you apply the chain rule in reverse order from the output back to the inputs:\n",
    "\n",
    "1. **Output Layer Gradient**:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial y_K} = \\text{Derivative of Loss function w.r.t. the output of the last layer}\n",
    "   $$\n",
    "\n",
    "2. **Backpropagation Through Layers**:\n",
    "   For each layer $k$ from $K$ down to 1, compute:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial z_k} = \\frac{\\partial L}{\\partial y_k} \\cdot f_k'(z_k)\n",
    "   $$\n",
    "   where $ f_k'(z_k) $ is the derivative of the activation function at layer $k$.\n",
    "\n",
    "   If $ k < K $, then:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial y_k} = \\frac{\\partial L}{\\partial z_{k+1}} W_{k+1}^T\n",
    "   $$\n",
    "\n",
    "3. **Gradient w.r.t. Weights**:\n",
    "   For each layer $ k $, compute:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W_k} = \\frac{\\partial L}{\\partial z_k} x_k^T\n",
    "   $$\n",
    "\n",
    "### Summary\n",
    "\n",
    "The backward pass effectively determines how the weights in each layer should be adjusted to minimize the loss, accounting for how each layer's output influences the loss through subsequent layers. Each $ \\frac{\\partial L}{\\partial W_k} $ points in the direction of greatest increase of the loss as a function of the weights in layer $ k $. By updating the weights in the opposite direction of this gradient, we move towards reducing the loss, implementing the essence of gradient descent.\n",
    "\n",
    "These derivations are used to compute how we handle backpropagation in our MLP example in  the `modeling/` directory. [network.py](../noojidiff/modeling/network.py) shows an example of how you can forward pass through the network, and how one can backwards pass the gradients through an MLP with ReLU activation and Negative Log-likelihood loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
