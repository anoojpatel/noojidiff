{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refreshers on Derivatives of Vector Valued Functions\n",
    "\n",
    "A Function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is differentiable at $a \\in \\mathbb{R}^n$ if there is an $m \\times n$ matrix such that:\n",
    "\n",
    "$$ \\lim_{x \\to a} \\frac{|f(x) - f(a) - A \\cdot (x-a) |} {|x-a|} = 0$$ \n",
    "\n",
    "If such matrix exists, the matrix $A$ is denoted by $Df(a)$ and is called the Jacobian.\n",
    "\n",
    "Note that $|x-a|$ is the distance metric defined by the Uclidean Distance $\\sqrt{{(x_1 - a_1)}^2 + {(x_2 - a_2)}^2 .. {(x_n-a_n)}^2}$ and is a real valued scalar.\n",
    "\n",
    "Formally, this can be derived from the general definiton of a derviative:\n",
    "\n",
    "$$\n",
    "f'(a) = \\lim_{x\\to a} \\frac{f(x) - f(a)}{x-a}\n",
    "$$\n",
    "Where this is only true if and only if:\n",
    "\n",
    "$$\n",
    "0= \\lim_{x\\to a} \\frac{f(x) - f(a)}{x-a} - f'(a)\n",
    "$$\n",
    "\n",
    "which can be transformed to:\n",
    "\n",
    "$$\n",
    "0= \\lim_{x\\to a} \\frac{f(x) - f(a) - f'(a)(x-a)}{x-a}\n",
    "$$\n",
    "\n",
    "and thus the evaluated to the final distance of each numerator and denominator from the origin:\n",
    "\n",
    "$$\n",
    "0= \\lim_{x\\to a} \\frac{|f(x) - f(a) - f'(a)(x-a)|}{|x-a|}\n",
    "$$\n",
    "(Since the notion of divison of two vectors is silly)\n",
    "\n",
    "Here $f'(a)$ represents our Jacobian matrix in $m \\times n$ shape. Denote I refer to this matrix from now on as $Df(x)$, though in the example above\n",
    "it is being used to be evaluated at point $a$ in vector space.\n",
    "\n",
    "\n",
    "### Defining the Jacobian in terms of coordinates and Indices\n",
    "\n",
    "#### Definitions of Jacobians via multiple functions of $f$\n",
    "Let the function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ be given by the m differentiable functions $f_1(x_,..,x_n),\\dots, f_m(x_1,\\dots, x_n)$ such that:\n",
    "\n",
    "$$\n",
    "f(x_1,\\dots,x_n) = \\begin{bmatrix}\n",
    "f_1((x_,..,x_n)) \\\\\n",
    "\\vdots \\\\\n",
    "\\vdots \\\\\n",
    "f_m((x_,..,x_n))\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Supposing we can represent each $f$ as a family of functions, indexed from 1 to $m$, we can take the derivative of each function $f_i$ for $i \\in 1\\dots m$:\n",
    "\n",
    "$$\n",
    "Df_i(x_1,...,x_n) \\to \\hat{v_i} \\ \\text{such that} \\ v_i \\in \\mathbb{R}^n\n",
    "$$\n",
    "In this case, we know $v_i$ to be $n$ dimensional, because of our original formulation of the derivative of vector valued functions. Note that we must compute\n",
    "$f'(a)$ which is another function $Df(a)$. However, we need the rows of $f'(a)$ (a linear map of sorts) for each ith row in $A$ to represent a tangent line. This \n",
    "tangent line is conditioned to be for an input vector valued to be resultant vector of $x-a$. \n",
    "\n",
    "Similar to our single valued derivative case (grade school calculus):\n",
    "$$\n",
    "y = f(a) + f'(a)(x-a)\n",
    "$$ \n",
    "where the above function is the tangent line of some differentiable function at some point $a$ for a function $y=f(x)$ (Note this function is *linear*),\n",
    "\n",
    "we want to build this same representation for a multivalued function $f_i(x_1,...,x_n)$. Thus we rely on partial derivatives to represent individual *linear* tangent lines\n",
    "with respect to each individual input $x_j$ where $j \\in 1...n$. Thus, we can represent each row of $Df(x)$ as:\n",
    "$$\n",
    "Df_i(x_1,..., x_n) = \\left (\\frac{\\partial f_i}{\\partial x_1} ,\\frac{\\partial f_i}{\\partial x_2}, \\cdots, \\frac{\\partial f_i}{\\partial x_n}\\right)\n",
    "$$\n",
    "So as a result, we can say $Df_i(x_1,...,x_n)$ when applied to the elements of $x-a$, will represent the linear approximations of wiggles on the vector valued function $f$ that will \n",
    "be approximately zero in distance from if we took the difference of $f(x) - f(a)$ exactly.\n",
    "\n",
    "As a result, we can expand this out to each function $f_1...f_m$ in $f$ so that $Df(x_1,...,x_n)$ our Jacobian is:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "Df(x) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Defining coordinates from domain and codomain dimensions\n",
    "In our case, we will want to have a conveinient notation for handling indices in our Jacobian matrix, and tracking our output vector's dimensions, along with input vector's dimensions.\n",
    "\n",
    "### Defining Differentials of Compositions of Functions\n",
    "\n",
    "Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ and $g: \\mathbb{R}^m \\rightarrow \\mathbb{R}^l$ be differentiable functions. Also there is a composition function:\n",
    "$$\n",
    "g \\circ f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^l\n",
    "$$\n",
    "that is also differentiable with a derivative given by: if $f(a)=b$, then\n",
    "$$\n",
    "Df(g\\circ f)(a) = D(g)(b) \\cdot D(f)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network from an MLP with ReLU and NLL Loss\n",
    "\n",
    "Multi-Layer Perceptron (MLP) with ReLU (Rectified Linear Unit) activation are common a neural network architecture, along with \n",
    "a Negative Log-likelihood loss (NLL Loss) for handling multiclass output.\n",
    "\n",
    "##### What does an MLP represent mathematically?\n",
    "\n",
    "An MLP is a neural network architecture where we have layers of preceptrons (sometimes called neurons) which can be seen as   \n",
    "connected groups of bipartite graphs, such that each layer of nodes (neurons) is not connected to any other node in it's layer. Each neuron recieves  \n",
    "inputs from all neurons from the layer previous to it. More specifically, each neuron in each layer takes all of the neuron inputs and multiplies  \n",
    "itself with a weight for each incoming input. These are all summed together. Each neuron in the layer then passes its summed value to  \n",
    "an activation function (such as the ReLU in our example). This will be our neuron's final output.\n",
    "\n",
    "### Let's Define it Better!\n",
    "\n",
    "As noted, we often model it as a series of layers where each layer   \n",
    "is composed of neurons. Each neuron computes a weighted sum of its inputs, which is then passed through an activation function.   \n",
    "Here's a simplified LaTeX representation that captures these components across multiple layers:\n",
    "\n",
    "Consider an MLP with one hidden layer and an output layer. The model consists of an input vector, hidden layers with activation functions, and an output layer. We will denote:\n",
    "- $ \\mathbf{x} $ as the input vector.\n",
    "- $ \\mathbf{W}^{(l)} $  as the weights of layer \\( l \\), respectively.\n",
    "- $ \\sigma $ as the activation function (e.g., sigmoid, ReLU).\n",
    "\n",
    "#### Hidden Layer\n",
    "\n",
    "Let's say the MLP has one hidden layer. The output of this layer for each neuron \\( j \\) in the hidden layer can be represented as:\n",
    "$$\n",
    "a_j^{(1)} = \\sigma\\left(\\sum_{i=1}^{n} W_{ji}^{(1)} x_i\\right)\n",
    "$$\n",
    "where $ n $ is the number of inputs to each neuron, $ W_{ji}^{(1)} $ are the weights connecting input $ i $ to neuron $ j $ in the first hidden layer.\n",
    "\n",
    "#### Output Layer\n",
    "\n",
    "For the output layer, if we have $ k $ outputs, the output for each neuron $k$ in the output layer can be represented as:\n",
    "$$\n",
    "y_k = \\sigma\\left(\\sum_{j=1}^{m} W_{kj}^{(2)} a_j^{(1)} \\right)\n",
    "$$\n",
    "where $ m $ is the number of neurons in the hidden layer, $ W_{kj}^{(2)} $ are the weights connecting the hidden layer neuron $j$ to the output neuron $k$.\n",
    "\n",
    "\n",
    "Notice that $(j)$ represents the logit value for a neuron, which is a row of the matrix $\\bm({W}^{(l)}$. If we iterate over every input neuron we can store  \n",
    "each value in a vector $[a_1, a_2, a_3.., a_k]$. This can thus be represented as simple a linear transformation! We will show more in the next section.\n",
    "\n",
    "**To Underscore**, we will reference this indexing later in this writeup. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Defining Operations Concretely\n",
    "\n",
    "Suppose you have an input $\\mathbf{x}$, weights $ \\mathbf{W} $, (and biases whidh is common but we leave it out $ \\mathbf{b} $). The operations in the layer can be described as:\n",
    "\n",
    "1. **Linear Transformation**: $ \\mathbf{z} = \\mathbf{Wx}$\n",
    "2. **ReLU Activation**: $ \\mathbf{a} = \\text{ReLU}(\\mathbf{z}) $, where $ \\text{ReLU}(z_i) = \\max(z_i, 0) $\n",
    "3. **NLL Loss**: $ \\mathbf{L} = \\text{NLLLoss}(\\mathbf{z}) = -\\log(z_t) $ \n",
    "\n",
    "From our previous example, we can cleanly represent our Input to Output as a Linear Transformation, rather than a combersome summation.  \n",
    " We will reference this from now on for the forward pass of MLP, and come back to this notation later when things get more tricky.\n",
    "\n",
    "### Example of a Forward Pass\n",
    "We will show an example of a forward pass that works on a simple network, with two layers, and two neurons, three inputs and  \n",
    "two outputs.\n",
    "\n",
    "Assume for our example, we have: \\\n",
    "  - **Input** $\\bm{x}$ = $[ 0, 1, 2]$\n",
    "  - **Weights**:\n",
    "    - $\\bm{W_1} = \\left[\\begin{smallmatrix}1&2\\\\3&4\\\\5&6\\end{smallmatrix}\\right]$\n",
    "    - $\\bm{W_2} = \\left[\\begin{smallmatrix}1&0\\\\0&1\\end{smallmatrix}\\right]$\n",
    "- **Label**: $t = 3$\n",
    "\n",
    "\n",
    "#### The Forward Pass\n",
    "\n",
    "1. **Layer 1**: Input $ \\mathbf{x} $, weights $ \\mathbf{W}_1 $\n",
    "   - Linear transformation: $ \\mathbf{z}_1 = \\mathbf{W}_1 \\mathbf{x}  $\n",
    "   - Activation: $ \\mathbf{a}_1 = \\text{ReLU}(\\mathbf{z}_1) $\n",
    "2. **Output Layer**: Input: $ \\mathbf{a_1} $, weights $ \\mathbf{W}_2 $\n",
    "   - Linear transformation: $ \\mathbf{z_2} = \\mathbf{W}_2 \\mathbf{a}_1 $\n",
    "   - Activation: $ \\mathbf{a}_2 = \\text{ReLU}(\\mathbf{z}_2) $\n",
    "3. **Computing the Loss**: Input activations $ \\mathbf{a_3}$, labels $\\mathbf{t}$\n",
    "   - Loss computation: $\\mathbf{L} = -\\log(\\mathbf{a}_t) \\text{\\ where \\ } \\mathbf{a}_t = \\text{element at index \\ } t \\text{\\ of \\ } \\mathbf{a} $\n",
    "   - $t$ here represents the class label index of the input example. As a result the possible values must \n",
    "    be an index into the shape of $\\mathbf{a_3}$. e.g. if $\\mathbf{a}_3 \\in \\mathbb{R}^3$, $t \\in 1...3$\n",
    "\n",
    "### Quality of Life Reformulations\n",
    "Given how that we have set up the premise, we can go ahead and try to use our previous mathematical foundation to reformulate some specific parts of our forward pass and the operations themselves. Why do we do this? Well we want to make it easier to convince oneself of how these models update or learn (more on this in the [gradients and backpropogation](#backward-pass-reverse-mode) section)\n",
    "\n",
    "#### Representing MLP transformations in a more convenient Way\n",
    "\n",
    "We currently represent each linear transformation for each layer as a Matrix-Vector multiplication. We find this quite cumbersome to handle\n",
    "when we want to do operations later on, specifically when trying to differentiate our composed function calls.\n",
    "\n",
    "We can represent a linear transformation $ \\mathbf{z}_l = \\mathbf{W}_l \\mathbf{x}  $ as:\n",
    "$$\n",
    "f_{W_l}(\\bm{x}) \\ \\text{where \\ } f_{\\bm{W}_l}: \\mathbb{R}^n \\to \\mathbb{R}^m\n",
    "$$\n",
    "\n",
    "Now, our function is not necessarily multivalued, but represents a frozen transformation on vector valued inputs.\n",
    "Looking back to our refresher on vector valued functions, this means we can focus on differentiating $f$, the linear transformation directly.\n",
    "This also means we can invoke this function for inputs that are not just vectors, but matrices or tensors, representing collections of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass in a neural network, particularly for a Multi-Layer Perceptron (MLP) with ReLU (Rectified Linear Unit) activation, involves computing the gradient of the loss function with respect to the weights and biases. This process allows for the updating of parameters via optimization algorithms like gradient descent. Here's how it works for a MLP layer with ReLU activation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Gradients of our MLP\n",
    "\n",
    "### Backward Pass (Reverse Mode)\n",
    "\n",
    "The gradients need to be computed with respect to $ \\mathbf{W} $. Assuming that the gradient of the loss $ L $ with respect to the output of this layer $ \\mathbf{a} $ is known (denoted as $ \\frac{\\partial L}{\\partial \\mathbf{a}} $), the gradients can be calculated as follows:\n",
    "\n",
    "1. **Gradient through ReLU**:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{z}} = \\frac{\\partial L}{\\partial \\mathbf{a}} \\odot \\text{ReLU}'(\\mathbf{z})\n",
    "   $$\n",
    "   Here, $\\text{ReLU}'(z_i)$ is the derivative of ReLU, which is 1 for $ z_i > 0 $ and 0 otherwise.\n",
    "\n",
    "2. **Gradient w.r.t. Weights**:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{z}} \\cdot \\mathbf{x}^T\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{z}} \\quad (\\text{summing over the batch if needed})\n",
    "   $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
