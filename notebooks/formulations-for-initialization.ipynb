{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Initializing Weights of the MLP\n",
    "\n",
    "The initialization of neural network weights using a standard deviation of $\\sqrt{1/n}$ (where $n$ is the number of input neurons, also known as fan-in) is a strategy designed to maintain the variance of the outputs of each neuron at initialization. Let's delve into a mathematical explanation and derivation of why this specific value is chosen, particularly in the context of keeping the variance of the outputs stable.\n",
    "\n",
    "### Background\n",
    "When we initialize the weights of a neural network, we want to ensure that the signal (i.e., the output of each neuron before applying the activation function) does not vanish or explode as it propagates through the network. This stability helps in maintaining effective gradient propagation during training.\n",
    "\n",
    "### Assumptions\n",
    "- The weights $w_{ij}$ are initialized independently from a normal distribution with mean 0 and standard deviation $\\sigma$.\n",
    "- Each neuron receives $n$ inputs $ x_i $, which are also assumed to be independent and have a mean of 0 and some constant variance (say, variance = 1 for simplicity).\n",
    "\n",
    "### Output Variance Calculation\n",
    "Consider a neuron's output $ z $ before applying the activation function, calculated as:\n",
    "$ z = \\sum_{i=1}^n w_i x_i $\n",
    "where  $w_i $ are the weights and $ x_i $ are the inputs.\n",
    "\n",
    "#### Step 1: Calculate the Variance of $ z $\n",
    "Since the weights and inputs are independent and assuming the inputs also have zero mean, the variance of the product $ w_i x_i $ for each $ i $ is simply the product of their variances (due to independence and zero means):\n",
    "$ \\text{Var}(w_i x_i) = \\text{Var}(w_i) \\cdot \\text{Var}(x_i) = \\sigma^2 \\cdot 1 = \\sigma^2 $ (as we noted for simplicity $x_i$ has $\\text{Var}(x_i)=1$)\n",
    "\n",
    "Since $ z $ is the sum of $ n $ such independent terms $ w_i x_i $, the variance of $ z $ is the sum of their variances:\n",
    "$ \\text{Var}(z) = \\text{Var}\\left(\\sum_{i=1}^n w_i x_i\\right) = \\sum_{i=1}^n \\text{Var}(w_i x_i) = n\\sigma^2 $\n",
    "\n",
    "#### Step 2: Desired Variance of $ z $\n",
    "To maintain the variance of the output $ z $ similar to the variance of the input across layers, we would like $\\text{Var}(z) = 1$. This condition helps prevent the vanishing or exploding gradients during training.\n",
    "\n",
    "Setting $\\text{Var}(z) = 1 $:\n",
    "$ n\\sigma^2 = 1 $\n",
    "$ \\sigma^2 = \\frac{1}{n} $\n",
    "\n",
    "Therefore, the standard deviation $ \\sigma $ should be:\n",
    "$ \\sigma = \\sqrt{\\frac{1}{n}} $\n",
    "\n",
    "### Conclusion\n",
    "This derivation shows that setting the standard deviation of the weight initialization to $\\sqrt{1/n}$ ensures that the output of each neuron has a variance of 1, assuming the inputs also have a variance of 1. This balance is crucial for maintaining effective learning, as it prevents the scale of the neuron outputs from increasing or decreasing dramatically across layers, which can lead to numerical instability or poor convergence. This is why the $ \\sqrt{1/n} $ factor is commonly used in weight initialization methods like Xavier/Glorot initialization (which adjusts the variance further based on both the number of inputs and outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proving the indentities of linear transformations under Random Variables\n",
    "\n",
    "Let's go through the mathematical proof for how the linear transformations of a random variable affect its mean and variance. The transformation we are considering is $ Y = aX + b $, where $ X $ is a random variable with mean $ \\mu_X $ and variance $ \\sigma_X^2 $, and $ a $ and $ b $ are constants.\n",
    "\n",
    "### 1. Expectation (Mean)\n",
    "\n",
    "The expectation operator \\( E \\) has the properties of linearity, which means that for any constants $ a $ and $ b $, and a random variable $ X $:\n",
    "$ E[aX + b] = aE[X] + b $\n",
    "\n",
    "#### Proof for Mean\n",
    "Given that $ X $ has a mean of $ \\mu_X $, the mean of $ Y $ is calculated as follows:\n",
    "$$ E[Y] = E[aX + b] $$\n",
    "$$ E[Y] = aE[X] + b  $$\n",
    "$$ E[Y] = a\\mu_X + b $$\n",
    "\n",
    "Thus, the mean of $ Y $ is $ a\\mu_X + b $.\n",
    "\n",
    "### 2. Variance\n",
    "\n",
    "Variance, denoted as $ \\text{Var} $, measures the spread of a random variable around its mean. The variance of a transformed random variable $ Y = aX + b $ is defined as:\n",
    "$ \\text{Var}(Y) = E[(Y - E[Y])^2] $\n",
    "\n",
    "#### Proof for Variance\n",
    "Substituting $ Y = aX + b $ and $ E[Y] = a\\mu_X + b $ into the variance formula:\n",
    "$$ \\text{Var}(Y) = E[(aX + b - (a\\mu_X + b))^2] $$\n",
    "$$ \\text{Var}(Y) = E[(aX - a\\mu_X)^2] $$\n",
    "$$ \\text{Var}(Y) = E[a^2(X - \\mu_X)^2] $$\n",
    "$$ \\text{Var}(Y) = a^2E[(X - \\mu_X)^2] $$\n",
    "\n",
    "Since $ E[(X - \\mu_X)^2] $ is the definition of $ \\text{Var}(X) $, or $ \\sigma_X^2 $:\n",
    "$ \\text{Var}(Y) = a^2\\sigma_X^2 $\n",
    "\n",
    "#### Key Insight\n",
    "The addition of a constant $ b $ shifts the mean but does not affect the spread or variability of the distribution, hence it does not influence the variance. The multiplication by $ a $, however, scales the spread of the distribution by $ a^2 $.\n",
    "\n",
    "### Summary\n",
    "This proof shows that the mean and variance of a linear transformation of a random variable $ Y = aX + b $ are $ a\\mu_X + b $ and $ a^2\\sigma_X^2 $ respectively. These properties are foundational in probability and statistics and are extensively utilized across fields like data science, economics, and engineering to understand and predict the behavior of complex systems based on simpler underlying distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It Together\n",
    "\n",
    "### Applying the Transformation\n",
    "\n",
    "Given $ Y = aX + b $, we can substitute $a$ with our target standard deviation $\\sigma_T$.\n",
    "\n",
    "$$ Y  = \\sigma_T X $$\n",
    "\n",
    "We know now that:\n",
    "$ \\text{Var}(Y) = \\sigma_Y^2 = \\sigma^2\\sigma_X^2 $ and by square root, we get:\n",
    "$$ \\sigma_Y=\\sqrt{Var(Y)} = \\sqrt{\\sigma_T^2\\sigma_X^2} = \\sigma_T\\sigma_X $$\n",
    "$$ = \\sigma_T * 1 $$\n",
    "We substitute $sqrt{1/n}$ and our origin standard deviation $1$ to get: $ \\sigma_Y = \\sqrt{1/n} * 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
